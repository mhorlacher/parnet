{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# from torchrbpnet.layers import Conv1DFirstLayer, Conv1DResBlock, IndexEmbeddingOutputHead\n",
    "# from torchrbpnet.losses import MultinomialNLLLossFromLogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class Conv1DFirstLayer(nn.Module):\n",
    "    def __init__(self, in_chan, filters=128, kernel_size=12):\n",
    "        super(Conv1DFirstLayer, self).__init__()\n",
    "\n",
    "        self.conv1d = nn.Conv1d(in_chan, filters, kernel_size=kernel_size, padding='same')\n",
    "        self.act = nn.ReLU()\n",
    "    \n",
    "    def forward(self, inputs, **kwargs):\n",
    "        x = self.conv1d(inputs)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "# %%\n",
    "class Conv1DResBlock(nn.Module):\n",
    "    def __init__(self, in_chan, filters=128, kernel_size=3, dropout=0.25, dilation=1, residual=True):\n",
    "        super(Conv1DResBlock, self).__init__()\n",
    "\n",
    "        self.conv1d = nn.Conv1d(in_chan, filters, kernel_size=kernel_size, dilation=dilation, padding='same')\n",
    "        self.batch_norm = nn.BatchNorm1d(filters)\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.residual = residual\n",
    "    \n",
    "    def forward(self, inputs, **kwargs):\n",
    "        x = self.conv1d(inputs)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        if self.residual:\n",
    "            x = inputs + x\n",
    "        return x\n",
    "\n",
    "# %%\n",
    "class IndexEmbeddingOutputHead(nn.Module):\n",
    "    def __init__(self, n_tasks, dims):\n",
    "        super(IndexEmbeddingOutputHead, self).__init__()\n",
    "\n",
    "        # protein/experiment embedding of shape (p, d)\n",
    "        self.embedding = torch.nn.Embedding(n_tasks, dims)\n",
    "    \n",
    "    def forward(self, bottleneck, **kwargs):\n",
    "        # bottleneck of shape (batch, d, n) --> (batch, n, d)\n",
    "        bottleneck = torch.transpose(bottleneck, -1, -2)\n",
    "        \n",
    "        # embedding of (batch, p, d) --> (batch, d, p)\n",
    "        embedding = torch.transpose(self.embedding.weight, 0, 1)\n",
    "\n",
    "        logits = torch.matmul(bottleneck, embedding) # torch.transpose(self.embedding.weight, 0, 1)  \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, tasks, nlayers=9):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.tasks = tasks\n",
    "\n",
    "        self.body = nn.Sequential(*[Conv1DFirstLayer(4, 128)]+[(Conv1DResBlock(128, dilation=(2**i))) for i in range(nlayers)])\n",
    "        self.head = IndexEmbeddingOutputHead(len(self.tasks), dims=128)\n",
    "    \n",
    "    def forward(self, inputs, **kwargs):\n",
    "        x = inputs\n",
    "\n",
    "        for layer in self.body:\n",
    "            x = layer(x)\n",
    "\n",
    "        return self.head(x)\n",
    "network = Network(tasks=list(range(223)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "torch.Size([64, 4, 1000])\n",
      "torch.Size([64, 1000, 223])\n"
     ]
    }
   ],
   "source": [
    "from bioflow import io\n",
    "import tensorflow as tf\n",
    "\n",
    "class TFIterableDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, filepath, features_filepath=None, batch_size=64, cache=True, shuffle=None):\n",
    "        super(TFIterableDataset).__init__()\n",
    "\n",
    "        self.dataset = io.dataset_ops.load_tfrecord(filepath, deserialize=False)\n",
    "\n",
    "        # cache\n",
    "        if cache:\n",
    "            self.dataset = self.dataset.cache()\n",
    "\n",
    "        if shuffle:\n",
    "            self.dataset = self.dataset.shuffle(shuffle)\n",
    "\n",
    "        # deserialize\n",
    "        if features_filepath is None:\n",
    "            features_filepath = filepath + '.features.json'\n",
    "        self.features = io.dataset_ops.features_from_json_file(features_filepath)\n",
    "        self.dataset = io.dataset_ops.deserialize_dataset(self.dataset, self.features)\n",
    "\n",
    "        # batch\n",
    "        self.dataset = self.dataset.batch(batch_size)\n",
    "\n",
    "        # format dataset\n",
    "        self.dataset = self.dataset.map(lambda e: (tf.transpose(e['inputs']['input'], perm=[0, 2, 1]), tf.transpose(e['outputs']['signal']['total'], perm=[0, 2, 1])))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for example in self.dataset.as_numpy_iterator():\n",
    "            yield tf.nest.map_structure(lambda x: torch.tensor(x).to(torch.float32), example)\n",
    "\n",
    "dataset = TFIterableDataset('example-data-matrix/windows.chr13.4.data.matrix.filtered.tfrecord', shuffle=1_000_000)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=None)\n",
    "example = next(iter(dataloader))\n",
    "print(example[0].shape)\n",
    "print(example[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNLLLossFromLogits(nn.Module):\n",
    "    def __init__(self, reduction=torch.mean):\n",
    "        super(MultinomialNLLLossFromLogits, self).__init__()\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def __call__(self, y, y_pred, dim=-1):\n",
    "        neg_log_probs = self.log_likelihood_from_logits(y, y_pred, dim) * -1\n",
    "        if self.reduction is not None:\n",
    "            return self.reduction(neg_log_probs)\n",
    "        return neg_log_probs\n",
    "\n",
    "    def log_likelihood_from_logits(self, y, y_pred, dim):\n",
    "        return torch.sum(torch.mul(torch.log_softmax(y_pred, dim=dim), y), dim=dim) + self.log_combinations(y, dim)\n",
    "\n",
    "    def log_combinations(self, input, dim):\n",
    "        total_permutations = torch.lgamma(torch.sum(input, dim=dim) + 1)\n",
    "        counts_factorial = torch.lgamma(input + 1)\n",
    "        redundant_permutations = torch.sum(counts_factorial, dim=dim)\n",
    "        return total_permutations - redundant_permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1000, 223])\n",
      "torch.Size([64, 1000, 223])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(194.2709, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = example[1]\n",
    "print(y.shape)\n",
    "y_pred = network(example[0])\n",
    "print(y_pred.shape)\n",
    "\n",
    "loss_fn = MultinomialNLLLossFromLogits()\n",
    "loss_fn(y, y_pred, dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiRBPNet(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = Network(tasks=range(223))\n",
    "        self.loss_fn = MultinomialNLLLossFromLogits()\n",
    "\n",
    "    def training_step(self, batch, **kwargs):\n",
    "        x, y = batch\n",
    "        y_pred = self.network(x)\n",
    "        loss = self.loss_fn(y, y_pred, dim=-2)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "model = MultiRBPNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:176: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name    | Type                         | Params\n",
      "---------------------------------------------------------\n",
      "0 | network | Network                      | 481 K \n",
      "1 | loss_fn | MultinomialNLLLossFromLogits | 0     \n",
      "---------------------------------------------------------\n",
      "481 K     Trainable params\n",
      "0         Non-trainable params\n",
      "481 K     Total params\n",
      "1.928     Total estimated model params size (MB)\n",
      "/home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 0it [00:00, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (233) must match the size of tensor b (223) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=605'>606</a>\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=606'>607</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=607'>608</a>\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=608'>609</a>\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=609'>610</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py?line=35'>36</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py?line=36'>37</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py?line=37'>38</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py?line=39'>40</a>\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py?line=40'>41</a>\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=642'>643</a>\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=643'>644</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=644'>645</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=645'>646</a>\u001b[0m     ckpt_path,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=646'>647</a>\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=647'>648</a>\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=648'>649</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=649'>650</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=651'>652</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=652'>653</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1107'>1108</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1109'>1110</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1111'>1112</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1113'>1114</a>\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1114'>1115</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1191\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1188'>1189</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1189'>1190</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1190'>1191</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1214\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1210'>1211</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1212'>1213</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1213'>1214</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=196'>197</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=197'>198</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=198'>199</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=199'>200</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=200'>201</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:267\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py?line=264'>265</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(dataloader, batch_to_device\u001b[39m=\u001b[39mbatch_to_device)\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py?line=265'>266</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py?line=266'>267</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=196'>197</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=197'>198</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=198'>199</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=199'>200</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=200'>201</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:213\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=209'>210</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=211'>212</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=212'>213</a>\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_loop\u001b[39m.\u001b[39;49mrun(kwargs)\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=214'>215</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=216'>217</a>\u001b[0m \u001b[39m# update non-plateau LR schedulers\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=217'>218</a>\u001b[0m \u001b[39m# update epoch-interval ones only when we are at the end of training epoch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=196'>197</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=197'>198</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=198'>199</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=199'>200</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=200'>201</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:88\u001b[0m, in \u001b[0;36mTrainingBatchLoop.advance\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py?line=83'>84</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py?line=84'>85</a>\u001b[0m     optimizers \u001b[39m=\u001b[39m _get_active_optimizers(\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py?line=85'>86</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizer_frequencies, kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mbatch_idx\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py?line=86'>87</a>\u001b[0m     )\n\u001b[0;32m---> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py?line=87'>88</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_loop\u001b[39m.\u001b[39;49mrun(optimizers, kwargs)\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py?line=88'>89</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py?line=89'>90</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_loop\u001b[39m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=196'>197</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=197'>198</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=198'>199</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=199'>200</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=200'>201</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:202\u001b[0m, in \u001b[0;36mOptimizerLoop.advance\u001b[0;34m(self, optimizers, kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=198'>199</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madvance\u001b[39m(\u001b[39mself\u001b[39m, optimizers: List[Tuple[\u001b[39mint\u001b[39m, Optimizer]], kwargs: OrderedDict) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=199'>200</a>\u001b[0m     kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_kwargs(kwargs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_idx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hiddens)\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=201'>202</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_optimization(kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizers[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptim_progress\u001b[39m.\u001b[39;49moptimizer_position])\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=202'>203</a>\u001b[0m     \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=203'>204</a>\u001b[0m         \u001b[39m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=204'>205</a>\u001b[0m         \u001b[39m# would be skipped otherwise\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=205'>206</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_idx] \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39masdict()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:249\u001b[0m, in \u001b[0;36mOptimizerLoop._run_optimization\u001b[0;34m(self, kwargs, optimizer)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=240'>241</a>\u001b[0m         closure()\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=242'>243</a>\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=243'>244</a>\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=244'>245</a>\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=245'>246</a>\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=246'>247</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=247'>248</a>\u001b[0m     \u001b[39m# the `batch_idx` is optional with inter-batch parallelism\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=248'>249</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(optimizer, opt_idx, kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mbatch_idx\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m), closure)\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=250'>251</a>\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=252'>253</a>\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=253'>254</a>\u001b[0m     \u001b[39m# if no result, user decided to skip optimization\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=254'>255</a>\u001b[0m     \u001b[39m# otherwise update running loss + reset accumulated loss\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=255'>256</a>\u001b[0m     \u001b[39m# TODO: find proper way to handle updating running loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:370\u001b[0m, in \u001b[0;36mOptimizerLoop._optimizer_step\u001b[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=361'>362</a>\u001b[0m     rank_zero_deprecation(\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=362'>363</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe NVIDIA/apex AMP implementation has been deprecated upstream. Consequently, its integration inside\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=363'>364</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m PyTorch Lightning has been deprecated in v1.9.0 and will be removed in v2.0.0.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=366'>367</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m return True.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=367'>368</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=368'>369</a>\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39musing_native_amp\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprecision_plugin, MixedPrecisionPlugin)\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=369'>370</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=370'>371</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39moptimizer_step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=371'>372</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=372'>373</a>\u001b[0m     batch_idx,\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=373'>374</a>\u001b[0m     optimizer,\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=374'>375</a>\u001b[0m     opt_idx,\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=375'>376</a>\u001b[0m     train_step_and_backward_closure,\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=376'>377</a>\u001b[0m     on_tpu\u001b[39m=\u001b[39;49m\u001b[39misinstance\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49maccelerator, TPUAccelerator),\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=377'>378</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=378'>379</a>\u001b[0m     using_lbfgs\u001b[39m=\u001b[39;49mis_lbfgs,\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=379'>380</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=381'>382</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=382'>383</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1356\u001b[0m, in \u001b[0;36mTrainer._call_lightning_module_hook\u001b[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1352'>1353</a>\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1354'>1355</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1355'>1356</a>\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1357'>1358</a>\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1358'>1359</a>\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/core/module.py:1742\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_lbfgs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/core/module.py?line=1662'>1663</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimizer_step\u001b[39m(\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/core/module.py?line=1663'>1664</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/core/module.py?line=1664'>1665</a>\u001b[0m     epoch: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/core/module.py?line=1670'>1671</a>\u001b[0m     using_lbfgs: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/core/module.py?line=1671'>1672</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/core/module.py?line=1672'>1673</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/core/module.py?line=1673'>1674</a>\u001b[0m \u001b[39m    Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/core/module.py?line=1674'>1675</a>\u001b[0m \u001b[39m    each optimizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/core/module.py?line=1739'>1740</a>\u001b[0m \n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/core/module.py?line=1740'>1741</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/core/module.py?line=1741'>1742</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:169\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py?line=165'>166</a>\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py?line=167'>168</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py?line=168'>169</a>\u001b[0m step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_strategy\u001b[39m.\u001b[39;49moptimizer_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_idx, closure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py?line=170'>171</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_after_step()\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py?line=172'>173</a>\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:234\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py?line=231'>232</a>\u001b[0m \u001b[39m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py?line=232'>233</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule)\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py?line=233'>234</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision_plugin\u001b[39m.\u001b[39;49moptimizer_step(\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py?line=234'>235</a>\u001b[0m     optimizer, model\u001b[39m=\u001b[39;49mmodel, optimizer_idx\u001b[39m=\u001b[39;49mopt_idx, closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py?line=235'>236</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:119\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[0;34m(self, optimizer, model, optimizer_idx, closure, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=116'>117</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=117'>118</a>\u001b[0m closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_closure, model, optimizer, optimizer_idx, closure)\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=118'>119</a>\u001b[0m \u001b[39mreturn\u001b[39;00m optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/optim/optimizer.py?line=137'>138</a>\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/optim/optimizer.py?line=138'>139</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/optim/optimizer.py?line=139'>140</a>\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/optim/optimizer.py?line=140'>141</a>\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/optim/optimizer.py?line=141'>142</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/optim/optimizer.py?line=20'>21</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/optim/optimizer.py?line=21'>22</a>\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/optim/optimizer.py?line=22'>23</a>\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/optim/optimizer.py?line=23'>24</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/optim/optimizer.py?line=24'>25</a>\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/optim/adam.py:183\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/optim/adam.py?line=180'>181</a>\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/optim/adam.py?line=181'>182</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/optim/adam.py?line=182'>183</a>\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/optim/adam.py?line=184'>185</a>\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/optim/adam.py?line=185'>186</a>\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:105\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[0;34m(self, model, optimizer, optimizer_idx, closure)\u001b[0m\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=91'>92</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_closure\u001b[39m(\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=92'>93</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=93'>94</a>\u001b[0m     model: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=96'>97</a>\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=97'>98</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=98'>99</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=99'>100</a>\u001b[0m \u001b[39m    ``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=100'>101</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=101'>102</a>\u001b[0m \u001b[39m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=102'>103</a>\u001b[0m \u001b[39m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=103'>104</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=104'>105</a>\u001b[0m     closure_result \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=105'>106</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_closure(model, optimizer, optimizer_idx)\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=106'>107</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:149\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=147'>148</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=148'>149</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclosure(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=149'>150</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:135\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=133'>134</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosure\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ClosureResult:\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=134'>135</a>\u001b[0m     step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_fn()\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=136'>137</a>\u001b[0m     \u001b[39mif\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=137'>138</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarning_cache\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:419\u001b[0m, in \u001b[0;36mOptimizerLoop._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=409'>410</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=410'>411</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=411'>412</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=415'>416</a>\u001b[0m \u001b[39m    A ``ClosureResult`` containing the training step output.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=416'>417</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=417'>418</a>\u001b[0m \u001b[39m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=418'>419</a>\u001b[0m training_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(\u001b[39m\"\u001b[39;49m\u001b[39mtraining_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=419'>420</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mpost_training_step()\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=421'>422</a>\u001b[0m model_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_lightning_module_hook(\u001b[39m\"\u001b[39m\u001b[39mtraining_step_end\u001b[39m\u001b[39m\"\u001b[39m, training_step_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1494\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1490'>1491</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1492'>1493</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1493'>1494</a>\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1495'>1496</a>\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1496'>1497</a>\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:378\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py?line=375'>376</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mtrain_step_context():\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py?line=376'>377</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, TrainingStep)\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py?line=377'>378</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtraining_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m, in \u001b[0;36mMultiRBPNet.training_step\u001b[0;34m(self, batch, **kwargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m      9\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork(x)\n\u001b[0;32m---> 10\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m, in \u001b[0;36mMultinomialNLLLossFromLogits.__call__\u001b[0;34m(self, y, y_pred, dim)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, y_pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 7\u001b[0m     neg_log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_likelihood_from_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction(neg_log_probs)\n",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m, in \u001b[0;36mMultinomialNLLLossFromLogits.log_likelihood_from_logits\u001b[0;34m(self, y, y_pred, dim)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_likelihood_from_logits\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, y_pred, dim):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msum(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39mdim) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_combinations(y, dim)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (233) must match the size of tensor b (223) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=2)\n",
    "trainer.fit(model=model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c2a629b5d736a8b2a3c0111829bdedfa4bd0b48e49067d38bd73bb54a8250f9"
  },
  "kernelspec": {
   "display_name": "Python 3.10.9 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
