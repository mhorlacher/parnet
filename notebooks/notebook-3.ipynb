{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/marc/PhD/TorchRBPNet/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "print(os.getcwd())\n",
    "\n",
    "from torchrbpnet.data import tfrecord_to_dataloader, dummy_dataloader\n",
    "from torchrbpnet.data import datasets\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(datasets.TFIterableDataset('../example/data.matrix/head.tfrecord', batch_size=2, shuffle=100), batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrbpnet import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchrbpnet.layers.stems.StemConv1D"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers.StemConv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gin\n",
    "import gin.torch.external_configurables\n",
    "\n",
    "@gin.configurable()\n",
    "def train(network, max_epochs, optimizer):\n",
    "    pass\n",
    "\n",
    "gin.parse_config_file('../configs/hyparam.search/config.render-001.gin')\n",
    "\n",
    "b = layers.RNAConv1dBody()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/conv.py:309: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:895.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 1001])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b(torch.rand(1, 4, 1001)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import torchmetrics\n",
    "from torchrbpnet.losses import MultinomialNLLLossFromLogits\n",
    "from torchrbpnet.metrics import MultinomialNLLFromLogits #BatchedPCC\n",
    "from torchrbpnet.networks import MultiRBPNet\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, network, metrics=None, optimizer=torch.optim.Adam):\n",
    "        super().__init__()\n",
    "        self.network = network\n",
    "        self.loss_fn = MultinomialNLLLossFromLogits()\n",
    "        \n",
    "        # metrics\n",
    "        if metrics is None:\n",
    "            self.metrics = nn.ModuleDict({})\n",
    "        else:\n",
    "            self.metrics = nn.ModuleDict(metrics)\n",
    "        \n",
    "        # optimizer\n",
    "        self.optimizer_cls = optimizer\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.network(*args, **kwargs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.optimizer_cls(self.parameters())\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx, **kwargs):\n",
    "        inputs, y = batch\n",
    "        y = y['total']\n",
    "        y_pred = self.forward(inputs)\n",
    "        loss = self.loss_fn(y, y_pred, dim=-2)\n",
    "        self.compute_and_log_metics(y_pred, y, partition='train')\n",
    "        return loss\n",
    "    \n",
    "    def training_epoch_end(self, *args, **kwargs):\n",
    "        self._reset_metrics()\n",
    "\n",
    "    def validation_epoch_end(self, *args, **kwargs):\n",
    "        self._reset_metrics()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, y = batch\n",
    "        y = y['total']\n",
    "        y_pred = self.forward(inputs)\n",
    "        self.compute_and_log_metics(y_pred, y, partition='val')\n",
    "    \n",
    "    def compute_and_log_metics(self, y_pred, y, partition=None):\n",
    "        on_step = False\n",
    "        if partition == 'train':\n",
    "            on_step = True\n",
    "\n",
    "        for name, metric in self.metrics.items():\n",
    "            metric(y_pred, y)\n",
    "            self.log(f'{partition}/{name}', metric.compute(), on_step=on_step, on_epoch=True, prog_bar=False)\n",
    "    \n",
    "    def _reset_metrics(self):\n",
    "        for metric in self.metrics.values():\n",
    "            metric.reset()\n",
    "\n",
    "model = Model(network=MultiRBPNet(n_tasks=223))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (network): MultiRBPNet(\n",
       "    (body): RNAConv1dBody(\n",
       "      (stem): StemConv1D(\n",
       "        (conv1d): Conv1d(4, 128, kernel_size=(12,), stride=(1,), padding=same)\n",
       "        (batch_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): ReLU()\n",
       "      )\n",
       "      (tower): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): ResConv1DBlock(\n",
       "            (conv1d): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
       "            (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): SiLU()\n",
       "            (dropout): Dropout1d(p=0.15, inplace=False)\n",
       "            (linear_upsample): Conv1d(128, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          )\n",
       "          (1): ResConv1DBlock(\n",
       "            (conv1d): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
       "            (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): SiLU()\n",
       "            (dropout): Dropout1d(p=0.15, inplace=False)\n",
       "          )\n",
       "          (2): ResConv1DBlock(\n",
       "            (conv1d): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same, dilation=(2,))\n",
       "            (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): SiLU()\n",
       "            (dropout): Dropout1d(p=0.15, inplace=False)\n",
       "          )\n",
       "          (3): ResConv1DBlock(\n",
       "            (conv1d): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same, dilation=(3,))\n",
       "            (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): SiLU()\n",
       "            (dropout): Dropout1d(p=0.15, inplace=False)\n",
       "          )\n",
       "          (4): ResConv1DBlock(\n",
       "            (conv1d): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same, dilation=(5,))\n",
       "            (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): SiLU()\n",
       "            (dropout): Dropout1d(p=0.15, inplace=False)\n",
       "          )\n",
       "          (5): ResConv1DBlock(\n",
       "            (conv1d): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same, dilation=(7,))\n",
       "            (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): SiLU()\n",
       "            (dropout): Dropout1d(p=0.15, inplace=False)\n",
       "          )\n",
       "          (6): ResConv1DBlock(\n",
       "            (conv1d): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same, dilation=(11,))\n",
       "            (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): SiLU()\n",
       "            (dropout): Dropout1d(p=0.15, inplace=False)\n",
       "          )\n",
       "          (7): ResConv1DBlock(\n",
       "            (conv1d): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same, dilation=(17,))\n",
       "            (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): SiLU()\n",
       "            (dropout): Dropout1d(p=0.15, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (linear_projection): LinearProjectionConv1D(\n",
       "        (pointwise): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (head): IndexEmbeddingOutputHead(\n",
       "      (embedding): Embedding(223, 256)\n",
       "    )\n",
       "  )\n",
       "  (loss_fn): MultinomialNLLLossFromLogits()\n",
       "  (metrics): ModuleDict()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mods = [x for x in model.modules()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 256, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mods[-5].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:176: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name    | Type                         | Params\n",
      "---------------------------------------------------------\n",
      "0 | network | MultiRBPNet                  | 1.6 M \n",
      "1 | loss_fn | MultinomialNLLLossFromLogits | 0     \n",
      "2 | metrics | ModuleDict                   | 0     \n",
      "---------------------------------------------------------\n",
      "1.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 M     Total params\n",
      "6.439     Total estimated model params size (MB)\n",
      "/home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loggers/tensorboard.py:188: UserWarning: Could not log computational graph to TensorBoard: The `model.example_input_array` attribute is not set or `input_array` was not given.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (256x1000 and 256x223)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m bar \u001b[38;5;241m=\u001b[39m RichProgressBar()\n\u001b[1;32m     18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(default_root_dir\u001b[38;5;241m=\u001b[39mroot_log_dir, max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, logger\u001b[38;5;241m=\u001b[39mloggers, callbacks\u001b[38;5;241m=\u001b[39m[checkpoint_callback, early_stop_callback, LearningRateMonitor(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m, log_momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)])\n\u001b[0;32m---> 19\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mnetwork, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=605'>606</a>\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=606'>607</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=607'>608</a>\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=608'>609</a>\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=609'>610</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py?line=35'>36</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py?line=36'>37</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py?line=37'>38</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py?line=39'>40</a>\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py?line=40'>41</a>\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=642'>643</a>\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=643'>644</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=644'>645</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=645'>646</a>\u001b[0m     ckpt_path,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=646'>647</a>\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=647'>648</a>\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=648'>649</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=649'>650</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=651'>652</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=652'>653</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1107'>1108</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1109'>1110</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1111'>1112</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1113'>1114</a>\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1114'>1115</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1191\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1188'>1189</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1189'>1190</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1190'>1191</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1204\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1200'>1201</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_training_routine()\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1202'>1203</a>\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1203'>1204</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1205'>1206</a>\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1206'>1207</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1276\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1273'>1274</a>\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1274'>1275</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1275'>1276</a>\u001b[0m     val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1277'>1278</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1279'>1280</a>\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=196'>197</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=197'>198</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=198'>199</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=199'>200</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=200'>201</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:152\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=149'>150</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_dataloaders \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=150'>151</a>\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdataloader_idx\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataloader_idx\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=151'>152</a>\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher, dl_max_batches, kwargs)\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=153'>154</a>\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=154'>155</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=196'>197</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=197'>198</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=198'>199</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=199'>200</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py?line=200'>201</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:137\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=133'>134</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=135'>136</a>\u001b[0m \u001b[39m# lightning module methods\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=136'>137</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=137'>138</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step_end(output)\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=139'>140</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:234\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=222'>223</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"The evaluation step (validation_step or test_step depending on the trainer's state).\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=223'>224</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=224'>225</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=230'>231</a>\u001b[0m \u001b[39m    the outputs of the step\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=231'>232</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=232'>233</a>\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=233'>234</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(hook_name, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=235'>236</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1494\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1490'>1491</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1492'>1493</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1493'>1494</a>\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1495'>1496</a>\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1496'>1497</a>\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py?line=387'>388</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[1;32m    <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py?line=388'>389</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, ValidationStep)\n\u001b[0;32m--> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py?line=389'>390</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[8], line 50\u001b[0m, in \u001b[0;36mModel.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     48\u001b[0m inputs, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     49\u001b[0m y \u001b[38;5;241m=\u001b[39m y[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 50\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_and_log_metics(y_pred, y, partition\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1189'>1190</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1190'>1191</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1191'>1192</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1192'>1193</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1193'>1194</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1194'>1195</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1195'>1196</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torchrbpnet/networks.py:40\u001b[0m, in \u001b[0;36mMultiRBPNet.forward\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torchrbpnet/networks.py?line=34'>35</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbody(x)\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torchrbpnet/networks.py?line=35'>36</a>\u001b[0m \u001b[39m# # transpose: # (batch_size, dim, N) --> (batch_size, N, dim)\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torchrbpnet/networks.py?line=36'>37</a>\u001b[0m \u001b[39m# x = torch.transpose(x, dim0=-2, dim1=-1)\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torchrbpnet/networks.py?line=37'>38</a>\u001b[0m \u001b[39m# x = self.linear_projection(x)\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torchrbpnet/networks.py?line=39'>40</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhead(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1189'>1190</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1190'>1191</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1191'>1192</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1192'>1193</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1193'>1194</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1194'>1195</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1195'>1196</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torchrbpnet/layers/embedding.py:23\u001b[0m, in \u001b[0;36mIndexEmbeddingOutputHead.forward\u001b[0;34m(self, bottleneck, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torchrbpnet/layers/embedding.py?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, bottleneck, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torchrbpnet/layers/embedding.py?line=16'>17</a>\u001b[0m     \u001b[39m# bottleneck of shape (batch_size, N, dim)\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torchrbpnet/layers/embedding.py?line=17'>18</a>\u001b[0m     \u001b[39m# bottleneck = torch.transpose(bottleneck, -1, -2) # This was moved to the network module\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torchrbpnet/layers/embedding.py?line=18'>19</a>\u001b[0m     \n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torchrbpnet/layers/embedding.py?line=19'>20</a>\u001b[0m     \u001b[39m# embedding of (batch, p, d) --> (batch, d, p)\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torchrbpnet/layers/embedding.py?line=20'>21</a>\u001b[0m     embedding \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtranspose(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding\u001b[39m.\u001b[39mweight, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torchrbpnet/layers/embedding.py?line=22'>23</a>\u001b[0m     logits \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(bottleneck, embedding) \u001b[39m# torch.transpose(self.embedding.weight, 0, 1)  \u001b[39;00m\n\u001b[1;32m     <a href='file:///home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/torchrbpnet/layers/embedding.py?line=23'>24</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m logits\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (256x1000 and 256x223)"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, RichProgressBar, LearningRateMonitor\n",
    "\n",
    "root_log_dir = f'logs/{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "loggers = [\n",
    "    pl_loggers.TensorBoardLogger(root_log_dir+'/tensorboard', name='', version='', log_graph=True),\n",
    "    # pl_loggers.CSVLogger(root_log_dir+'/tensorboard', name='', version=''),\n",
    "]\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=f'{root_log_dir}/checkpoints', every_n_epochs=1, save_last=True)\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val/loss\", min_delta=0.00, patience=3, verbose=False, mode=\"min\")\n",
    "\n",
    "bar = RichProgressBar()\n",
    "\n",
    "trainer = pl.Trainer(default_root_dir=root_log_dir, max_epochs=2, logger=loggers, callbacks=[checkpoint_callback, early_stop_callback, LearningRateMonitor('step', log_momentum=True)])\n",
    "trainer.fit(model=model, train_dataloaders=dataloader, val_dataloaders=dataloader)\n",
    "torch.save(model.network, 'test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizers()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c2a629b5d736a8b2a3c0111829bdedfa4bd0b48e49067d38bd73bb54a8250f9"
  },
  "kernelspec": {
   "display_name": "Python 3.10.9 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
